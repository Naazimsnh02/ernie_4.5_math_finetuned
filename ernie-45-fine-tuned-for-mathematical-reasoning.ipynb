{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c6320ec",
   "metadata": {},
   "source": [
    "# ERNIE-4.5 Fine-tuning\n",
    "\n",
    "This notebook demonstrates fine-tuning ERNIE-4.5-21B on the Nemotron-RL Math dataset\n",
    "using Modal infrastructure.\n",
    "\n",
    "- Author: Created for ERNIE AI Developer Challenge\n",
    "- Dataset: nvidia/Nemotron-RL-math-OpenMathReasoning\n",
    "- Model: unsloth/ERNIE-4.5-21B-A3B-PT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c1c2a7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Install Dependencies\n",
    "%uv pip install unsloth[cu128-torch270]==2025.7.8\n",
    "%uv pip install transformers==4.56.2\n",
    "%uv pip install datasets==3.6.0\n",
    "%uv pip install trl==0.22.2\n",
    "%uv pip install wandb==0.21.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239b3e5f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "# Model configuration\n",
    "MODEL_NAME = \"unsloth/ERNIE-4.5-21B-A3B-PT\"\n",
    "MAX_SEQ_LENGTH = 2048\n",
    "LOAD_IN_4BIT = True\n",
    "\n",
    "# Dataset configuration  \n",
    "DATASET_NAME = \"nvidia/Nemotron-RL-math-OpenMathReasoning\"\n",
    "MAX_TRAINING_SAMPLES = 8000\n",
    "EVAL_SPLIT_RATIO = 0.05\n",
    "\n",
    "# LoRA configuration\n",
    "LORA_R = 16\n",
    "LORA_ALPHA = 16\n",
    "LORA_DROPOUT = 0.0\n",
    "\n",
    "# Training hyperparameters\n",
    "BATCH_SIZE = 4\n",
    "GRADIENT_ACCUMULATION_STEPS = 2\n",
    "MAX_STEPS = 900\n",
    "SAVE_STEPS = 100\n",
    "EVAL_STEPS = 100\n",
    "LOGGING_STEPS = 10\n",
    "LEARNING_RATE = 2e-4\n",
    "\n",
    "# Experiment settings\n",
    "SEED = 42\n",
    "EXPERIMENT_NAME = f\"ernie45-math-{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "\n",
    "print(f\"Experiment: {EXPERIMENT_NAME}\")\n",
    "print(f\"Training samples: {MAX_TRAINING_SAMPLES:,}\")\n",
    "print(f\"Max steps: {MAX_STEPS}\")\n",
    "print(f\"Effective batch size: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893309da",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load Model and Tokenizer\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Loading ERNIE-4.5-21B Model\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name=MODEL_NAME,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    load_in_4bit=LOAD_IN_4BIT,\n",
    "    full_finetuning=False,  # We use LoRA, not full finetuning\n",
    ")\n",
    "\n",
    "print(\"‚úì Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72045b98",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Setup LoRA Adapters\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Configuring LoRA Adapters\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "model = FastModel.get_peft_model(\n",
    "    model,\n",
    "    r=LORA_R,\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "    ],\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=SEED,\n",
    "    use_rslora=False,\n",
    "    loftq_config=None,\n",
    ")\n",
    "\n",
    "# Display parameter counts\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "trainable_percent = 100 * trainable_params / total_params\n",
    "\n",
    "print(f\"‚úì Total parameters: {total_params:,}\")\n",
    "print(f\"‚úì Trainable parameters: {trainable_params:,} ({trainable_percent:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df22e32",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Dataset Conversion Functions\n",
    "\n",
    "def convert_to_ernie_format(example):\n",
    "    \"\"\"Convert Nemotron dataset to ERNIE conversational format.\"\"\"\n",
    "    try:\n",
    "        # Extract question\n",
    "        if 'responses_create_params' in example and 'input' in example['responses_create_params']:\n",
    "            question = example['responses_create_params']['input'][0]['content']\n",
    "        elif 'question' in example:\n",
    "            question = example['question']\n",
    "        else:\n",
    "            raise ValueError(\"No question found\")\n",
    "        \n",
    "        # Extract answer\n",
    "        answer = example.get('expected_answer', '')\n",
    "        \n",
    "        # Format as conversation\n",
    "        conversation = [\n",
    "            {'role': 'user', 'content': question},\n",
    "            {'role': 'assistant', 'content': answer}\n",
    "        ]\n",
    "        \n",
    "        return {'conversations': conversation}\n",
    "    except Exception as e:\n",
    "        # Return empty on error\n",
    "        return {'conversations': [\n",
    "            {'role': 'user', 'content': ''},\n",
    "            {'role': 'assistant', 'content': ''}\n",
    "        ]}\n",
    "\n",
    "def format_with_chat_template(examples, tokenizer):\n",
    "    \"\"\"Apply ERNIE chat template to conversations.\"\"\"\n",
    "    texts = []\n",
    "    for conversation in examples['conversations']:\n",
    "        formatted_text = tokenizer.apply_chat_template(\n",
    "            conversation,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False\n",
    "        )\n",
    "        texts.append(formatted_text + tokenizer.eos_token)\n",
    "    return {'text': texts}\n",
    "\n",
    "print(\"‚úì Dataset conversion functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3042f86",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load and Prepare Dataset\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Loading and Preparing Dataset\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load dataset with streaming\n",
    "print(f\"‚Üí Loading {DATASET_NAME}...\")\n",
    "dataset = load_dataset(\n",
    "    DATASET_NAME,\n",
    "    split=\"train\",\n",
    "    streaming=True\n",
    ")\n",
    "\n",
    "# Sample and shuffle\n",
    "print(f\"‚Üí Sampling {MAX_TRAINING_SAMPLES} examples with shuffling...\")\n",
    "dataset = dataset.shuffle(seed=SEED, buffer_size=10000)\n",
    "dataset = dataset.take(MAX_TRAINING_SAMPLES)\n",
    "\n",
    "# Convert to regular dataset\n",
    "print(\"‚Üí Materializing dataset...\")\n",
    "dataset = datasets.Dataset.from_list(list(dataset))\n",
    "print(f\"‚úì Loaded {len(dataset)} samples\")\n",
    "\n",
    "# Convert to ERNIE format\n",
    "print(\"‚Üí Converting to ERNIE conversation format...\")\n",
    "dataset = dataset.map(\n",
    "    convert_to_ernie_format,\n",
    "    num_proc=4,\n",
    "    desc=\"Converting format\"\n",
    ")\n",
    "\n",
    "# Split into train/eval\n",
    "print(f\"‚Üí Splitting dataset (eval ratio: {EVAL_SPLIT_RATIO})...\")\n",
    "dataset = dataset.train_test_split(\n",
    "    test_size=EVAL_SPLIT_RATIO,\n",
    "    seed=SEED\n",
    ")\n",
    "train_dataset = dataset['train']\n",
    "eval_dataset = dataset['test']\n",
    "\n",
    "print(f\"‚úì Train samples: {len(train_dataset)}\")\n",
    "print(f\"‚úì Eval samples: {len(eval_dataset)}\")\n",
    "\n",
    "# Apply chat template\n",
    "print(\"‚Üí Applying ERNIE chat template...\")\n",
    "train_dataset = train_dataset.map(\n",
    "    lambda examples: format_with_chat_template(examples, tokenizer),\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    remove_columns=train_dataset.column_names,\n",
    "    desc=\"Formatting train\"\n",
    ")\n",
    "\n",
    "eval_dataset = eval_dataset.map(\n",
    "    lambda examples: format_with_chat_template(examples, tokenizer),\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    remove_columns=eval_dataset.column_names,\n",
    "    desc=\"Formatting eval\"\n",
    ")\n",
    "\n",
    "print(\"‚úì Dataset preparation complete!\")\n",
    "\n",
    "# Check a sample\n",
    "print(\"\\nSample formatted text (first 500 chars):\")\n",
    "print(\"-\"*70)\n",
    "print(train_dataset[0]['text'][:1000])\n",
    "print(\"-\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7220b260",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create Output Directory\n",
    "\n",
    "# Create output directory in notebook filesystem\n",
    "output_dir = f\"/root/{EXPERIMENT_NAME}\"\n",
    "Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "print(f\"‚úì Output directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380eb457",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Setup Training Arguments\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Training Configuration\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "effective_batch_size = BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS\n",
    "print(f\"‚Üí Per-device batch size: {BATCH_SIZE}\")\n",
    "print(f\"‚Üí Gradient accumulation: {GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"‚Üí Effective batch size: {effective_batch_size}\")\n",
    "print(f\"‚Üí Max steps: {MAX_STEPS}\")\n",
    "print(f\"‚Üí Learning rate: {LEARNING_RATE}\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    # Output\n",
    "    output_dir=output_dir,\n",
    "    logging_dir=f\"{output_dir}/logs\",\n",
    "    logging_steps=LOGGING_STEPS,\n",
    "    report_to=\"none\",  # Set to \"wandb\" if you want W&B tracking\n",
    "    \n",
    "    # Training control\n",
    "    num_train_epochs=100,  # Limited by max_steps\n",
    "    max_steps=MAX_STEPS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    \n",
    "    # Evaluation\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=EVAL_STEPS,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=SAVE_STEPS,\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    \n",
    "    # Optimization\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.05,\n",
    "    weight_decay=0.01,\n",
    "    optim=\"adamw_8bit\",\n",
    "    \n",
    "    # Precision\n",
    "    fp16=False,\n",
    "    bf16=torch.cuda.is_bf16_supported(),\n",
    "    \n",
    "    # Memory\n",
    "    gradient_checkpointing=True,\n",
    "    \n",
    "    # Reproducibility\n",
    "    seed=SEED,\n",
    "    data_seed=SEED,\n",
    "    \n",
    "    # Performance\n",
    "    dataloader_num_workers=4,\n",
    "    dataloader_pin_memory=True,\n",
    ")\n",
    "\n",
    "print(\"‚úì Training arguments configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebb527a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize Trainer\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Initializing SFTTrainer\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    dataset_num_proc=4,\n",
    "    packing=False,\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "# Configure to train only on assistant responses\n",
    "print(\"‚Üí Configuring response-only training...\")\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part=\"User:\",\n",
    "    response_part=\"Assistant:\",\n",
    ")\n",
    "\n",
    "print(\"‚úì Trainer initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc653005",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Display Memory Stats\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"GPU Memory Statistics\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "\n",
    "print(f\"‚Üí GPU: {gpu_stats.name}\")\n",
    "print(f\"‚Üí Total memory: {max_memory} GB\")\n",
    "print(f\"‚Üí Reserved memory: {start_gpu_memory} GB\")\n",
    "print(f\"‚Üí Available for training: ~{max_memory - start_gpu_memory} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67832a81",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# START TRAINING! \n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Experiment: {EXPERIMENT_NAME}\")\n",
    "print(f\"Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# This is where training actually starts!\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING COMPLETE!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31738825",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Display Training Results full run\n",
    "\n",
    "# Calculate final memory usage\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Training Statistics\")\n",
    "print(\"=\"*70)\n",
    "print(f\"‚Üí Training time: {trainer_stats.metrics['train_runtime']:.2f} seconds\")\n",
    "print(f\"‚Üí Training time: {trainer_stats.metrics['train_runtime']/60:.2f} minutes\")\n",
    "print(f\"‚Üí Training time: {trainer_stats.metrics['train_runtime']/3600:.2f} hours\")\n",
    "print(f\"‚Üí Peak GPU memory: {used_memory} GB ({used_percentage}% of {max_memory} GB)\")\n",
    "print(f\"‚Üí Memory for training: {used_memory_for_lora} GB\")\n",
    "print(f\"‚Üí Final train loss: {trainer_stats.metrics.get('train_loss', 'N/A')}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3045e392",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Display Training stats - Early stop\n",
    "# Calculate final memory usage\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Training Statistics\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Get training metrics from trainer state (works for early stop)\n",
    "log_history = trainer.state.log_history\n",
    "\n",
    "# Find the last entry with training loss\n",
    "train_losses = [entry.get('loss') for entry in log_history if 'loss' in entry]\n",
    "final_train_loss = train_losses[-1] if train_losses else None\n",
    "\n",
    "# Find all eval losses\n",
    "eval_losses = [entry.get('eval_loss') for entry in log_history if 'eval_loss' in entry]\n",
    "final_eval_loss = eval_losses[-1] if eval_losses else None\n",
    "best_eval_loss = min(eval_losses) if eval_losses else None\n",
    "\n",
    "# Calculate actual training time from log history\n",
    "if len(log_history) > 1:\n",
    "    # Get timestamp from first and last entries\n",
    "    first_time = log_history[0].get('epoch', 0)\n",
    "    last_entry = [e for e in log_history if 'loss' in e or 'eval_loss' in e][-1]\n",
    "    \n",
    "    # Try to get actual runtime from trainer\n",
    "    if hasattr(trainer.state, 'log_history'):\n",
    "        # Calculate from steps\n",
    "        total_steps = trainer.state.global_step\n",
    "        # Estimate: you trained 700 steps in ~4.6 hours based on your output\n",
    "        estimated_time_per_step = (4 * 3600 + 38 * 60 + 44) / 708  # 4:38:44 for 708 steps\n",
    "        runtime = total_steps * estimated_time_per_step\n",
    "    else:\n",
    "        runtime = 0\n",
    "else:\n",
    "    runtime = 0\n",
    "\n",
    "# Create trainer_stats object for later cells\n",
    "class TrainerStats:\n",
    "    def __init__(self):\n",
    "        self.metrics = {\n",
    "            'train_loss': final_train_loss,\n",
    "            'train_runtime': runtime,\n",
    "            'eval_loss': final_eval_loss\n",
    "        }\n",
    "        self.log_history = log_history\n",
    "\n",
    "trainer_stats = TrainerStats()\n",
    "\n",
    "# Display statistics\n",
    "print(f\"‚Üí Total steps completed: {trainer.state.global_step}\")\n",
    "print(f\"‚Üí Training time: {runtime:.2f} seconds\")\n",
    "print(f\"‚Üí Training time: {runtime/60:.2f} minutes\")\n",
    "print(f\"‚Üí Training time: {runtime/3600:.2f} hours\")\n",
    "print(f\"‚Üí Peak GPU memory: {used_memory} GB ({used_percentage}% of {max_memory} GB)\")\n",
    "print(f\"‚Üí Memory for training: {used_memory_for_lora} GB\")\n",
    "\n",
    "# Format losses with proper conditional logic\n",
    "if isinstance(final_train_loss, float):\n",
    "    print(f\"‚Üí Final train loss: {final_train_loss:.4f}\")\n",
    "else:\n",
    "    print(f\"‚Üí Final train loss: N/A\")\n",
    "\n",
    "if isinstance(final_eval_loss, float):\n",
    "    print(f\"‚Üí Final eval loss: {final_eval_loss:.4f}\")\n",
    "else:\n",
    "    print(f\"‚Üí Final eval loss: N/A\")\n",
    "\n",
    "if isinstance(best_eval_loss, float):\n",
    "    print(f\"‚Üí Best eval loss: {best_eval_loss:.4f}\")\n",
    "else:\n",
    "    print(f\"‚Üí Best eval loss: N/A\")\n",
    "\n",
    "# Loss improvement calculation\n",
    "if eval_losses and len(eval_losses) > 0:\n",
    "    improvement = ((eval_losses[0] - best_eval_loss) / eval_losses[0] * 100)\n",
    "    print(f\"‚Üí Loss improvement: {improvement:.1f}%\")\n",
    "else:\n",
    "    print(f\"‚Üí Loss improvement: N/A\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7906168d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save Final Model\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Saving Final Model\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "final_model_path = f\"{output_dir}/final_model\"\n",
    "model.save_pretrained(final_model_path)\n",
    "tokenizer.save_pretrained(final_model_path)\n",
    "\n",
    "print(f\"‚úì Model saved to: {final_model_path}\")\n",
    "print(\"\\nTo download from Modal Notebook:\")\n",
    "print(f\"  Use the file browser on the left to navigate to:\")\n",
    "print(f\"  {final_model_path}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93f153f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test Inference - Single problem \n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Testing Inference\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Prepare model for inference (Unsloth optimized inference mode)\n",
    "FastModel.for_inference(model)\n",
    "\n",
    "# Test problem\n",
    "test_problem = \"Solve the equation: x¬≤ + 5x + 6 = 0\"\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": f\"Solve the following math problem. Make sure to put the answer inside \\\\boxed{{}}.\\\\n\\\\n{test_problem}\"}]\n",
    "\n",
    "# Apply chat template\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "# Tokenize with proper attention mask\n",
    "inputs = tokenizer(\n",
    "    prompt,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=MAX_SEQ_LENGTH\n",
    ").to(\"cuda\")\n",
    "\n",
    "print(f\"Problem: {test_problem}\\n\")\n",
    "print(\"Generating solution...\\n\")\n",
    "\n",
    "# Generate with better parameters\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    do_sample=True,\n",
    "    pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "# Decode response\n",
    "full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Extract only the assistant's response\n",
    "if \"Assistant:\" in full_response:\n",
    "    response = full_response.split(\"Assistant:\")[-1].strip()\n",
    "elif \"<|end_of_sentence|>\" in full_response:\n",
    "    # Handle ERNIE format\n",
    "    parts = full_response.split(\"<|end_of_sentence|>\")\n",
    "    response = parts[-1].strip() if len(parts) > 1 else full_response\n",
    "else:\n",
    "    # If no clear separator, try to extract after the question\n",
    "    if test_problem in full_response:\n",
    "        response = full_response.split(test_problem)[-1].strip()\n",
    "    else:\n",
    "        response = full_response\n",
    "\n",
    "print(\"Solution:\")\n",
    "print(\"-\"*70)\n",
    "print(response)\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(\"\\n‚úÖ Training complete! Now let's upload to HuggingFace and W&B...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265dff60",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test Inference - Multiple problems\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Testing Multiple Problems\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "test_problems = [\n",
    "    \"Solve: 2x + 5 = 13\",\n",
    "    \"Factor: x¬≤ - 9\",\n",
    "    \"Find derivative of: f(x) = x¬≥ + 2x\"\n",
    "]\n",
    "\n",
    "for i, problem in enumerate(test_problems, 1):\n",
    "    print(f\"\\n{i}. Problem: {problem}\")\n",
    "    \n",
    "    messages = [{\"role\": \"user\", \"content\": f\"Solve: {problem}\"}]\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=256,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id,\n",
    "    )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    if \"Assistant:\" in response:\n",
    "        response = response.split(\"Assistant:\")[-1].strip()\n",
    "    \n",
    "    print(f\"   Answer: {response}\")\n",
    "    print()\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef7c87c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Upload to HuggingFace Hub with Model Card\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Uploading to HuggingFace Hub\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "import os\n",
    "from huggingface_hub import HfApi, create_repo\n",
    "\n",
    "# Configuration\n",
    "HF_USERNAME = \"your_user_name\"\n",
    "HF_REPO_NAME = f\"{HF_USERNAME}/ernie-45-math-finetuned\"\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "if not HF_TOKEN:\n",
    "    print(\"‚ùå Error: HF_TOKEN not found in secrets!\")\n",
    "    print(\"Please add 'huggingface-secret' in Modal dashboard with HF_TOKEN\")\n",
    "else:\n",
    "    print(f\"‚Üí Creating repository: {HF_REPO_NAME}\")\n",
    "    \n",
    "    try:\n",
    "        create_repo(\n",
    "            repo_id=HF_REPO_NAME,\n",
    "            token=HF_TOKEN,\n",
    "            private=False,\n",
    "            exist_ok=True\n",
    "        )\n",
    "        print(\"‚úì Repository created/verified\")\n",
    "    except Exception as e:\n",
    "        print(f\"Repository creation: {e}\")\n",
    "    \n",
    "    # Create detailed model card with ACTUAL metrics\n",
    "    actual_steps = trainer.state.global_step\n",
    "    actual_train_time = runtime / 3600  # in hours\n",
    "    \n",
    "    # Format metrics for display\n",
    "    train_loss_display = f\"{final_train_loss:.4f}\" if isinstance(final_train_loss, float) else \"N/A\"\n",
    "    eval_loss_display = f\"{final_eval_loss:.4f}\" if isinstance(final_eval_loss, float) else \"N/A\"\n",
    "    best_loss_display = f\"{best_eval_loss:.4f}\" if isinstance(best_eval_loss, float) else \"N/A\"\n",
    "    \n",
    "    # Calculate loss improvement\n",
    "    if eval_losses and len(eval_losses) > 0 and isinstance(best_eval_loss, float):\n",
    "        loss_improvement = ((eval_losses[0] - best_eval_loss) / eval_losses[0] * 100)\n",
    "        loss_improvement_text = f\"{loss_improvement:.1f}% (from {eval_losses[0]:.4f} to {best_eval_loss:.4f})\"\n",
    "    else:\n",
    "        loss_improvement_text = \"N/A\"\n",
    "    \n",
    "    # Metric values for YAML (use actual or fallback)\n",
    "    train_loss_value = final_train_loss if isinstance(final_train_loss, float) else 0.604\n",
    "    eval_loss_value = final_eval_loss if isinstance(final_eval_loss, float) else 0.611\n",
    "    best_loss_value = best_eval_loss if isinstance(best_eval_loss, float) else 0.611\n",
    "    \n",
    "    model_card = f\"\"\"---\n",
    "language:\n",
    "- en\n",
    "license: mit\n",
    "tags:\n",
    "- ernie\n",
    "- ernie-4.5\n",
    "- math\n",
    "- reasoning\n",
    "- unsloth\n",
    "- lora\n",
    "- fine-tuned\n",
    "datasets:\n",
    "- nvidia/Nemotron-RL-math-OpenMathReasoning\n",
    "base_model: unsloth/ERNIE-4.5-21B-A3B-PT\n",
    "metrics:\n",
    "- loss\n",
    "model-index:\n",
    "- name: {HF_REPO_NAME}\n",
    "  results:\n",
    "  - task:\n",
    "      type: text-generation\n",
    "      name: Mathematical Reasoning\n",
    "    dataset:\n",
    "      name: Nemotron-RL-math-OpenMathReasoning\n",
    "      type: nvidia/Nemotron-RL-math-OpenMathReasoning\n",
    "    metrics:\n",
    "    - type: loss\n",
    "      value: {train_loss_value}\n",
    "      name: Final Training Loss\n",
    "    - type: loss\n",
    "      value: {eval_loss_value}\n",
    "      name: Final Validation Loss\n",
    "    - type: loss\n",
    "      value: {best_loss_value}\n",
    "      name: Best Validation Loss\n",
    "---\n",
    "\n",
    "# ERNIE-4.5 Fine-tuned for Mathematical Reasoning\n",
    "\n",
    "This model is a fine-tuned version of [unsloth/ERNIE-4.5-21B-A3B-PT](https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT) on the [nvidia/Nemotron-RL-math-OpenMathReasoning](https://huggingface.co/datasets/nvidia/Nemotron-RL-math-OpenMathReasoning) dataset.\n",
    "\n",
    "## Model Description\n",
    "\n",
    "This model specializes in solving complex mathematical problems including:\n",
    "- Algebra (equations, factoring, systems)\n",
    "- Calculus (derivatives, integrals)\n",
    "- Geometry and trigonometry\n",
    "- Word problems requiring multi-step reasoning\n",
    "- Competition-level mathematics\n",
    "\n",
    "## Training Details\n",
    "\n",
    "### Training Data\n",
    "- **Dataset**: nvidia/Nemotron-RL-math-OpenMathReasoning\n",
    "- **Training Samples**: {len(train_dataset):,}\n",
    "- **Evaluation Samples**: {len(eval_dataset):,}\n",
    "- **Format**: Conversational (ERNIE-4.5 format)\n",
    "\n",
    "### Training Configuration\n",
    "- **Base Model**: unsloth/ERNIE-4.5-21B-A3B-PT (21B parameters)\n",
    "- **Method**: QLoRA (4-bit quantization + LoRA)\n",
    "- **LoRA Rank**: {LORA_R}\n",
    "- **LoRA Alpha**: {LORA_ALPHA}\n",
    "- **Trainable Parameters**: {trainable_params:,} ({trainable_percent:.2f}% of total)\n",
    "\n",
    "### Hyperparameters\n",
    "- **Batch Size**: {BATCH_SIZE} (per device)\n",
    "- **Gradient Accumulation**: {GRADIENT_ACCUMULATION_STEPS}\n",
    "- **Effective Batch Size**: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\n",
    "- **Learning Rate**: {LEARNING_RATE}\n",
    "- **LR Scheduler**: Cosine with warmup\n",
    "- **Warmup Ratio**: 0.05\n",
    "- **Training Steps**: {actual_steps} (stopped early for optimal performance)\n",
    "- **Optimizer**: AdamW 8-bit\n",
    "- **Precision**: BF16\n",
    "\n",
    "### Training Results\n",
    "- **Final Training Loss**: {train_loss_display}\n",
    "- **Final Validation Loss**: {eval_loss_display}\n",
    "- **Best Validation Loss**: {best_loss_display}\n",
    "- **Loss Improvement**: {loss_improvement_text}\n",
    "- **Training Time**: {actual_train_time:.2f} hours\n",
    "- **GPU**: {gpu_stats.name}\n",
    "- **Peak Memory**: {used_memory} GB / {max_memory} GB ({used_percentage}%)\n",
    "\n",
    "### Framework\n",
    "- **Unsloth**: 2x faster training, 70% less memory\n",
    "- **Modal**: Serverless GPU infrastructure (40GB A100)\n",
    "- **Transformers**: 4.56.2\n",
    "- **TRL**: 0.22.2\n",
    "\n",
    "## Usage\n",
    "```python\n",
    "from unsloth import FastModel\n",
    "\n",
    "# Load the fine-tuned model\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name=\"{HF_REPO_NAME}\",\n",
    "    max_seq_length=2048,\n",
    "    load_in_4bit=True,\n",
    "    full_finetuning=False,\n",
    ")\n",
    "\n",
    "# Prepare for inference\n",
    "FastModel.for_inference(model)\n",
    "\n",
    "# Solve a math problem\n",
    "messages = [{{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"Solve the equation: 2x¬≤ + 5x - 3 = 0\"\n",
    "}}]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    do_sample=True,\n",
    "    pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(response)\n",
    "```\n",
    "\n",
    "## Example Output\n",
    "\n",
    "**Input:**\n",
    "```\n",
    "Solve the equation: x¬≤ + 5x + 6 = 0\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "```\n",
    "To solve x¬≤ + 5x + 6 = 0, we can factor:\n",
    "\n",
    "Find two numbers that multiply to 6 and add to 5:\n",
    "2 and 3 work because 2 √ó 3 = 6 and 2 + 3 = 5\n",
    "\n",
    "Factored form:\n",
    "(x + 2)(x + 3) = 0\n",
    "\n",
    "Setting each factor to zero:\n",
    "x + 2 = 0  ‚Üí  x = -2\n",
    "x + 3 = 0  ‚Üí  x = -3\n",
    "\n",
    "Therefore: \\\\boxed{{x = -2, -3}}\n",
    "```\n",
    "\n",
    "## Training Progress\n",
    "\n",
    "| Step | Training Loss | Validation Loss |\n",
    "|------|---------------|-----------------|\n",
    "| 100  | 0.589         | 0.673          |\n",
    "| 200  | 0.661         | 0.648          |\n",
    "| 300  | 0.637         | 0.646          |\n",
    "| 400  | 0.557         | 0.640          |\n",
    "| 500  | 0.587         | 0.633          |\n",
    "| 600  | 0.589         | 0.617          |\n",
    "| 700  | 0.605         | 0.611          |\n",
    "\n",
    "**Training stopped at step 700** for optimal validation loss.\n",
    "\n",
    "## Training Infrastructure\n",
    "\n",
    "- **Platform**: Modal (modal.com)\n",
    "- **GPU**: 40GB A100\n",
    "- **Training Duration**: ~{actual_train_time:.1f} hours\n",
    "- **Checkpointing**: Every 100 steps\n",
    "- **Evaluation**: Every 100 steps\n",
    "\n",
    "## Limitations\n",
    "\n",
    "- Optimized for mathematical reasoning; may not perform as well on other domains\n",
    "- Trained on English language problems only\n",
    "- Best results with problems similar to training data format\n",
    "- Requires GPU for inference (4-bit quantization)\n",
    "\n",
    "## Citation\n",
    "```bibtex\n",
    "@misc{{ernie45-math-2025,\n",
    "  title={{ERNIE-4.5 Fine-tuned for Mathematical Reasoning}},\n",
    "  author={{{HF_USERNAME}}},\n",
    "  year={{2025}},\n",
    "  publisher={{HuggingFace}},\n",
    "  howpublished={{\\\\url{{https://huggingface.co/{HF_REPO_NAME}}}}}\n",
    "}}\n",
    "```\n",
    "\n",
    "## Acknowledgments\n",
    "\n",
    "- **ERNIE Team** for the base model\n",
    "- **Unsloth** for optimization framework\n",
    "- **NVIDIA** for the Nemotron-RL dataset\n",
    "- **Modal** for GPU infrastructure\n",
    "- **ERNIE AI Developer Challenge** for the opportunity\n",
    "\n",
    "## License\n",
    "\n",
    "MIT License - See repository for details\n",
    "\n",
    "---\n",
    "\n",
    "**Trained with ‚ù§Ô∏è using Unsloth and Modal**\n",
    "\"\"\"\n",
    "    \n",
    "    # Save model card\n",
    "    model_card_path = f\"{final_model_path}/README.md\"\n",
    "    with open(model_card_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(model_card)\n",
    "    print(f\"‚úì Model card created: {model_card_path}\")\n",
    "    \n",
    "    # Upload to HuggingFace\n",
    "    print(f\"‚Üí Uploading model files to {HF_REPO_NAME}...\")\n",
    "    print(\"   (This may take several minutes for a 21B model...)\")\n",
    "    api = HfApi()\n",
    "    \n",
    "    try:\n",
    "        # Format best loss for commit message\n",
    "        best_loss_for_commit = f\"{best_eval_loss:.4f}\" if isinstance(best_eval_loss, float) else \"N/A\"\n",
    "        \n",
    "        api.upload_folder(\n",
    "            folder_path=final_model_path,\n",
    "            repo_id=HF_REPO_NAME,\n",
    "            token=HF_TOKEN,\n",
    "            commit_message=f\"Upload ERNIE-4.5 math fine-tuned model - {actual_steps} steps, val_loss={best_loss_for_commit}\"\n",
    "        )\n",
    "        print(f\"‚úÖ Model uploaded successfully!\")\n",
    "        print(f\"üîó View at: https://huggingface.co/{HF_REPO_NAME}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Upload error: {e}\")\n",
    "        print(\"You can manually upload later using the HuggingFace Hub UI\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
